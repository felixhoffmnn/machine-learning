{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs. Dogs\n",
    "\n",
    "Um sich ein initiales Bild über die *state-of-the-art* Trainingsmethoden für den `Cats vs. Dogs` Datensatz zu verschaffen wurde sich an diesem [Beispiel](https://www.kaggle.com/code/uysimty/keras-cnn-dog-or-cat-classification) orientiert.\n",
    "\n",
    "**Matrikel-Nr.**: 1946566\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "- `tensorflow`\n",
    "- `tensorflow-datasets`\n",
    "- `keras` (zusätzlich zu tensorflow)\n",
    "- `pyyaml`\n",
    "- `h5py`\n",
    "- `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# Disable TF warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Decide if you want to train a new model or load an existing one\n",
    "LOAD_MODEL = True\n",
    "# The default number of epochs to train the model is `20`\n",
    "# For demonstration purposes and to safe time, I suggest to set it to `5`\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"./models\")\n",
    "\n",
    "if LOAD_MODEL and not Path(model_path / \"cnn-0.88.h5\").exists():\n",
    "    req = requests.get(\"https://github.com/felixhoffmnn/machine-learning/raw/main/machine_learning/exam/models/cnn-0.88.h5\")\n",
    "\n",
    "    with open(model_path / \"cnn-0.88.h5\", \"wb\") as f:\n",
    "        f.write(req.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Der `cats_vs_dogs` Datensatz besteht aus `23.262` Bildern von Katzen und Hunden. Die Bilder sind in 2 Klassen aufgeteilt: Katzen und Hunde (gelabelter Datensatz). Der Datensatz beinhaltet außerdem `1738` korrupte Bilder, die automatisch entfernt wurden.\n",
    "\n",
    "`split=[\"train[:80%]\", \"train[80%:]\"]` teilt den Datensatz in 2 Teile auf. Der erste Teil wird für das Training verwendet, der zweite Teil für die Validierung. Mittels `shuffle_files=True` werden die Bilder zufällig gemischt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(\"notebook\", font_scale=1.5, style=\"white\", rc={\"figure.figsize\":(20, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('CatsVsDogs', split=[\"train[:80%]\", \"train[80%:]\"], shuffle_files=True, as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_examples = tfds.visualization.show_examples(ds_train, ds_info, rows=4, cols=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "Wie sich aus der Recherche ergabt, gibt es verschiedene Möglichkeiten, die Bilder vorzubereiten. Im folgenden wenden wir drei Methoden an um das Bild zu normalisieren und zu skalieren. Außerdem werden die Bilder wenn nötig geshuffelt und in Batches aufgeteilt.\n",
    "\n",
    "Mittels der Konvertierung aus dem `RGB` Farbraum in den `Grayscale` Farbraum wird die Anzahl der Farbkanäle von 3 auf 1 reduziert. Im Anschluss normalisieren wir das Histogramm der Bilder indem wir die Pixelwerte auf den Bereich von 0 bis 1 skalieren. Abschließend skalieren wir die Bilder auf `128 x 128` Pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gray(image, label):\n",
    "    return tf.image.rgb_to_grayscale(image), label\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label  # type: ignore\n",
    "\n",
    "def resize_img(image, label):\n",
    "    return tf.image.resize(image, (128, 128)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(ds, batch_size=32, is_train=True):\n",
    "    ds = ds.map(convert_to_gray, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(resize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_train:\n",
    "        ds = ds.shuffle(1000)\n",
    "        \n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_prep = image_preprocessing(ds_train)\n",
    "ds_test_prep = image_preprocessing(ds_test, batch_size=64, is_train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import layers, models, optimizers, Sequential, regularizers\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = Path(\"./checkpoints\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Aus der Recherche ergabt sich, dass eines der häufigsten Modelle für die Klassifikation ein **VGG 3** ist. Dieses besteht aus drei Convolution Layers, welche nacheinander versuchen die Bilder auf Basis von Merkmalen und Features zu klassifizieren. Um letztlich einen Binären Output zu erhalten, wird zunächst ein Flatten Layer verwendet, gefolgt von zwei Dense Layers. Der erste von beiden verarbeitet die Daten mit 512 Neuronen, der zweite mit 1 Neuron. \n",
    "\n",
    "Nachdem das aktuelle Bild mittels einem Convolution Layer verarbeitet wurde, wird das Ergebnis normalisiert und mittels einer MaxPooling Funktion auf die Hälfte reduziert. Dieser Vorgang wird 3 mal wiederholt. Das MaxPooling ist außerdem gefolgt von Dropout Layers, welche das Overfitting verhindern sollen.\n",
    "\n",
    "**Iterationen**:\n",
    "\n",
    "- Die erste Version des Modells beinhaltete ausschließlich Convolution Layers, MaxPooling, Flatten und Dense Layers &rarr; Das Modell hat stark *Overfitted*\n",
    "- Das hinzufügen von Dropout Layers und Batch Normalization hat das Overfitting verhindert &rarr; Das Modell hat sich verbessert\n",
    "- Durch das hinzufügen von `kernel_initializer` konnte die Trainingsgeschwindigkeit erhöht werden\n",
    "- Padding vergrößert die `shape` &rarr; Das Modell hat sich verbessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_uniform\", padding=\"same\", input_shape=(128, 128, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_uniform\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_uniform\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(), loss=keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%d_%m-%H_%M\")\n",
    "\n",
    "checkpoint_path = checkpoint_path / f\"cats-dogs-{time}.ckpt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden **Callback-Funktionen** ermöglichen es das Training zu beenden, sobald sich die Genauigkeit nicht sonderlich verbessert. Außerdem verringern wir die `learning_rate` sobald die `validated_accuracy` nicht mehr steigt, sondern fällt. \n",
    "\n",
    "Durch die Verringerung der `learning_rate` konnten wir Genauigkeiten von bis zu `88` % erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, save_weights_only=True)\n",
    "stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, min_lr=0.00001) # type: ignore\n",
    "\n",
    "callbacks = [checkpoints, stop, lr_reduce]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnung**: Das Training des Modells kann sehr lange dauern (ca. 1 Stunde). Daher wurde das Modell bereits trainiert und gespeichert. Somit kann das Modell direkt geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    history = model.fit(ds_train_prep, validation_data=ds_test_prep, epochs=EPOCHS, callbacks=callbacks, use_multiprocessing=True)\n",
    "    \n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    print(history_df)\n",
    "    \n",
    "    highest = history_df[\"val_accuracy\"].max().round(2)\n",
    "    model.save(model_path / f\"cats-dogs-{time}-{highest}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "    sns.lineplot(x=history_df.index, y=history_df[\"loss\"], ax=axs[0], label=\"loss\")\n",
    "    sns.lineplot(x=history_df.index, y=history_df[\"val_loss\"], ax=axs[0], label=\"val_loss\")\n",
    "    sns.lineplot(x=history_df.index, y=history_df[\"accuracy\"], ax=axs[1], label=\"accuracy\")\n",
    "    sns.lineplot(x=history_df.index, y=history_df[\"val_accuracy\"], ax=axs[1], label=\"val_accuracy\")\n",
    "\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "\n",
    "    axs[0].set_xlabel(\"Epochen\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    axs[1].set_xlabel(\"Epochen\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laden eines Modells**: Mittels des folgenden Codes kann ein bereits trainiertes Modell geladen werden. Zunächst muss `LOAD_MODEL` auf `True` gesetzt werden. Anschließend wird noch gefragt, welches Modell geladen werden soll.\n",
    "\n",
    "Beim *Testen* des Ladens eines Modells wurde festgestellt, dass die Genauigkeit des Modells nicht identisch ist, wie als es trainiert wurde. Jedoch ist die Genauigkeit des Modells nach dem Laden noch immer sehr hoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    models = [file for file in os.listdir(model_path) if file.endswith(\".h5\")]\n",
    "    \n",
    "    if len(models) > 0:\n",
    "        for i, file in enumerate(models):\n",
    "            print(f\"{i}: {file}\")\n",
    "            \n",
    "        model_index = int(input(\"\\nSelect model: \"))\n",
    "        \n",
    "        if model_index in range(len(models)):\n",
    "            model = load_model(model_path / models[model_index])\n",
    "            \n",
    "            if model is not None:\n",
    "                print(f\"\\nLoaded model: {models[model_index]}\")\n",
    "                history = model.evaluate(ds_test_prep, verbose=1, use_multiprocessing=True)\n",
    "                \n",
    "                print(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In dieser Prüfungsleistung konnte ein `VGG3` mit einer Genauigkeit von `80 - 90` % trainiert werden. Als Datensatz wurde der **Cats vs. Dogs** Datensatz verwendet. Das Training wurde mittels eines `80:20` Split durchgeführt.\n",
    "\n",
    "Um das Modell weiter zu verbessern könne man die Trainingsdaten verzerren, drehen und vergrößern. Dadurch könnte das Modell besser an Abweichungen bei den Testdaten angepasst werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "00155b37fbb991365d0ee05c40a31a80e0b75de72a8bd7547540e678c5c3c0a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
